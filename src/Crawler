
import java.util.*;
import java.net.*;
import java.io.*;
import java.util.regex.*;

import org.apache.commons.httpclient.*;
import org.apache.commons.httpclient.methods.GetMethod;
import org.apache.commons.httpclient.params.HttpMethodParams;
/*
 * Usage:java Crawler seedUrl rootPath directoryName
 * args[0]: url seed
 * args[1]: rootPath 
 * args[2]: directory
 * 
 * java Crawler www.npr.org /Users/ejmnorl/Desktop HELLO
 */
public class Crawler 
{   
	
	String rootPath; //root path to store the downloaded pages
	String directoryName; //directory name to store download files
	String seed;//seed url to start searching
	String domain;//domain name

	public static void main(String[] args)
	{
		if (args.length != 3)
		{
			System.out.println("Usage:java Crawler seedUrl, rootPath, directoryName");
			return;
		}
		String seedToSearch = args[0];
		String rootPath = args[1];
		String directory = args[2];
		
		Crawler crawler = new Crawler(seedToSearch, rootPath, directory);
		
		System.out.println("The whole crawling process starting.......");
		crawler.crawl();
		System.out.println("Finally done!");         
	}
	
	
	public Crawler(String seed, String rootPath, String directory)
	{
		this.seed = seed;
		this.domain = seed.split("www.")[1].trim();
		this.rootPath = rootPath;
		this.directoryName = directory;
		createDirectoryForDownloadFiles();
	}
	
    /**
     *Crawl web pages within same domain of seed URL
     */
    public void crawl()	
    {
        HashSet<String> crawledList = new HashSet<String>();
        LinkedHashSet<String> toCrawlList = new LinkedHashSet<String>();
        
        toCrawlList.add(seed);
        while (toCrawlList.size() > 0)
        {                      
            // Get URL at bottom of the list.
            String url = toCrawlList.iterator().next();
            if (!url.contains(domain)) {//Only crawl websites within the same domain
            	continue;
            }                        
            // Retrieval URL from the to crawl list.
            toCrawlList.remove(url);            
            URL refUrl = geneUrl(url);
            crawledList.add(url);
            //Five-second delay
            try{
                Thread.sleep(5000);
            } catch(InterruptedException e){
                
            }            
            //download the file
            String pageUrlContent = downloadFile(refUrl.toString());						
            if (pageUrlContent != null && pageUrlContent.length() > 0)
            {
                // retrieve links from the page
                ArrayList<String> links = retrieveLinks(refUrl, pageUrlContent, crawledList);
                
                toCrawlList.addAll(links);               
            }            
        }
    }
	
    /**
     * check the format of retrieved url and generate url
     * @param string url
     * @return URL url
     */
	private URL geneUrl(String url)
	{
		URL generedUrl = null;
		try {
			if(!url.toLowerCase().contains("http://")){ 
				generedUrl = new URL("http://"+url);
			} else {
				generedUrl = new URL(url);
			}			
		} catch (Exception e) {
			return null;
		}
		System.out.println("url = " + url + " geneUrl = " + generedUrl.toString());
		return generedUrl;
	}
    
    /**
     * Retrieve links from a certain web page
     *
     * @param page url
     * @param content of this web page
     * @param record what pages has been crawled
     * @return page links referenced by this page 
     */
    private ArrayList<String> retrieveLinks(URL pageUrl, String pageContents, HashSet<String> crawledList)
    {         
        Pattern p = Pattern.compile("<a\\s+href\\s*=\\s*\"?(.*?)[\"|>]",Pattern.CASE_INSENSITIVE);  
        Matcher matcher = p.matcher(pageContents);  
        
        ArrayList<String> linkList = new ArrayList<String>();  
        while (matcher.find())  
        {  
            String link = matcher.group(1).trim();  
            
            if (link.length() < 1 || link.charAt(0) == '#' || link.toLowerCase().indexOf("javascript") != -1 )  
                continue;  
            
            if(link.indexOf(domain) == -1) continue;
            
            if(link.indexOf("://") == -1)  
            {  
                if (link.charAt(0) == '/'){          	  
                    link = "http://" + pageUrl.getHost() + ":" + pageUrl.getPort() + link;  
                } 
                else{
                    String file = pageUrl.getFile();  
                    if (file.indexOf('/') == -1){
                        link = "http://" + pageUrl.getHost() + ":" + pageUrl.getPort() + "/" + link;  
                    }
                    else{  
                        String path = file.substring(0,file.lastIndexOf('/') + 1);  
                        link = "http://" + pageUrl.getHost() + ":" + pageUrl.getPort() + path + link;  
                    }  
                }  
                
            }  
            
            if (link.indexOf('#') != -1)  
            {  
                link = link.substring(0, link.indexOf('#'));  
            }  
            
            URL verifiedLink = geneUrl(link);  
            if (verifiedLink == null)  
            	continue;
            
            // Skip the linked that has already handled to make sure all the urls retrieved are unique
            if (crawledList.contains(link)) 
            	continue;
            
            linkList.add(link);  
        }  
        
        return (linkList);  
    }  

    /**
     * Using HttpClient to download file and analysis the page type
     *
     * @param String url
     * @return Content of this page
     */
	private String  downloadFile(String url) 
	{
        String fileContent=null;
        HttpClient httpClient=new HttpClient();
        //Setting timeout to be 5s
        httpClient.getHttpConnectionManager().getParams().setConnectionTimeout(5000);
        
        GetMethod getMethod=new GetMethod(url);	 
        
        getMethod.getParams().setParameter(HttpMethodParams.SO_TIMEOUT,5000);
        getMethod.getParams().setParameter(HttpMethodParams.RETRY_HANDLER,
                                           new DefaultHttpMethodRetryHandler());
	
        File file = null;
        FileWriter fw = null;
        
        try{ 
			httpClient.executeMethod(getMethod);				 
			String pageName =  url.split("http://")[1].trim() + ".html";
			if (pageName.contains("/")) {
				String[] items = pageName.split("/");
				pageName = items[items.length - 1];
			}
            String fileName = rootPath + "/" + directoryName + "/" + pageName;
            System.out.println("real file name : " + fileName);
			file = new File(fileName);
			
			if (!file.exists()) {
				file.createNewFile();
			}
			fw = new FileWriter(file);
            BufferedReader in = new BufferedReader(new InputStreamReader(getMethod.getResponseBodyAsStream()));		  
           
            String res = "";  
            StringBuilder contentBuffer = new StringBuilder();
            while((res = in.readLine()) != null){  
                contentBuffer.append(res);  
            	fw.write(res);
            }  
            fileContent = contentBuffer.toString(); 
           
        } catch (HttpException e) {		
            e.printStackTrace();
        } catch (IOException e) {				   
            e.printStackTrace();
        } finally {				   
            getMethod.releaseConnection();	
            
        }
        return fileContent;
	}
	
    /**
     * Create directory for download files
     */
	private void createDirectoryForDownloadFiles() {
		File direc = null;
		direc = new File(rootPath + "/" + directoryName);
		if(direc.exists()) {
			deleteDirectory(direc);
			direc.delete();
		}
		if(new File(direc.toString()).mkdir()) {
			System.out.println(" Directory has been created");
		}
	}
	
    /**
     * Delete directory content
     */	
	private void deleteDirectory(File dir) {
		for(File f: dir.listFiles()) {
			if(f.isDirectory())
				deleteDirectory(f);
			else 
				f.delete();
		}
	}
    
    
}
